{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-28T16:58:51.819715Z",
     "start_time": "2024-07-28T16:58:45.457394Z"
    },
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "#Import necessary libraries\n",
    "\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from psycopg2.extras import execute_batch\n",
    "import psycopg2\n",
    "import requests\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime, timedelta\n",
    "import datetime\n",
    "import re\n",
    "from serpapi import GoogleSearch\n",
    "from datasets import Dataset\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException, WebDriverException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "#Connecting to the database\n",
    "\n",
    "load_dotenv()\n",
    "host = 'localhost'\n",
    "port = 5432 \n",
    "user = 'postgres'\n",
    "password = os.getenv('PASSWORD_DB')\n",
    "db = 'postgres' \n",
    "conn = psycopg2.connect(host=host, port=port, user=user, password=password, dbname=db)\n",
    "connection_string = f\"postgresql://{user}:{password}@{host}/{db}\"\n",
    "engine = create_engine(connection_string)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scrape news from CoinDesk\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "\n",
    "\n",
    "service = Service(ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "links = []\n",
    "xpaths = []\n",
    "\n",
    "for i in range(371):\n",
    "    base_url = f'https://www.coindesk.com/tag/ethereum/{i}/'\n",
    "    driver.get(base_url)\n",
    "    print(i)\n",
    "\n",
    "    for i in range(1,11):\n",
    "        xpaths.append(f'//*[@id=\"fusion-app\"]/div/div/div/main/div[2]/div/div[1]/div[{i}]') \n",
    "\n",
    "    for xpath in xpaths:\n",
    "        try:\n",
    "            articles_container = driver.find_element(By.XPATH, xpath)\n",
    "        \n",
    "            article_elements = articles_container.find_elements(By.XPATH, './/a')\n",
    "        \n",
    "            for article in article_elements:\n",
    "                href = article.get_attribute('href')\n",
    "                if href and href.startswith(\"https://www.coindesk.com/\"):\n",
    "                    links.append(href)\n",
    "        except Exception as e:\n",
    "            print(f\"Error finding elements with XPath {xpath}: {e}\")\n",
    "\n",
    "    for i, link in enumerate(links, start=1):\n",
    "        print(f\"Link {i}: {link}\")\n",
    "\n",
    "    xpaths.clear()\n",
    "\n",
    "\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_date(url):\n",
    "    return bool(re.search(r'\\d{4}/\\d{2}/\\d{2}', url))\n",
    "\n",
    "links_step_2 = [url for url in links if has_date(url)]\n",
    "links_end = list(set(links_step_2))\n",
    "links_end\n",
    "\n",
    "links_df= pd.DataFrame(links_end)\n",
    "links_df.to_sql(name = 'links_df'\n",
    "            , con = engine\n",
    "            , index = False\n",
    "            , if_exists ='replace')\n",
    "links_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links_dic = {\"text\": [], \"time\": []}\n",
    "\n",
    "def get_content(link):\n",
    "    try:\n",
    "        response = requests.get(link)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "\n",
    "        base_content = soup.find(attrs={\"data-module-name\": \"article-body-no-right-rail\"})\n",
    "        if base_content:\n",
    "            paragraphs = base_content.find_all('p')\n",
    "            all_text = \"\\n\".join(paragraph.get_text() for paragraph in paragraphs)\n",
    "        else:\n",
    "            all_text = \"Content not found\"\n",
    "\n",
    "\n",
    "        time_element = soup.find(class_=\"iOUkmj\")\n",
    "        time = time_element.get_text() if time_element else \"Time not found\"\n",
    "\n",
    "        links_dic['text'].append(all_text)\n",
    "        links_dic[\"time\"].append(time)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching content from {link}: {e}\")\n",
    "\n",
    "def get_contents(page_links):\n",
    "    for page_link in tqdm(page_links, desc=\"Fetching content\"):\n",
    "        get_content(page_link)\n",
    "\n",
    "\n",
    "get_contents(links_end)\n",
    "\n",
    "\n",
    "news = pd.DataFrame(links_dic)\n",
    "news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning and correcting data fetched from CoinDesk\n",
    "\n",
    "news = news[~news['text'].str.contains('Error fetching content from')]\n",
    "\n",
    "def convert_date(date_str):\n",
    "    date_str_cleaned = date_str.split(' UTC')[0].strip()\n",
    "    \n",
    "    date_str_cleaned = date_str_cleaned.replace('p.m.', 'PM').replace('a.m.', 'AM')\n",
    "    \n",
    "    formats = [\n",
    "        '%b %d, %Y at %I:%M %p',\n",
    "        '%b %d, %Y at %H:%M %p'\n",
    "    ]\n",
    "    \n",
    "    for fmt in formats:\n",
    "        try:\n",
    "            date_obj = datetime.strptime(date_str_cleaned, fmt)\n",
    "            return date_obj.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        except ValueError:\n",
    "            continue\n",
    "    return None\n",
    "\n",
    "news['time'] = news['time'].apply(convert_date)\n",
    "\n",
    "news.dropna(inplace=True)\n",
    "\n",
    "news['time'] = pd.to_datetime(news['time'], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>url</th>\n",
       "      <th>urlToImage</th>\n",
       "      <th>publishedAt</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'id': None, 'name': 'ReadWrite'}</td>\n",
       "      <td>Radek Zielinski</td>\n",
       "      <td>Ethereum launches $2M ‘Attackathon’ security a...</td>\n",
       "      <td>Ethereum’s (ETH) Ethereum Foundation is initia...</td>\n",
       "      <td>https://readwrite.com/ethereum-launches-2m-att...</td>\n",
       "      <td>https://readwrite.com/wp-content/uploads/2024/...</td>\n",
       "      <td>2024-07-09T09:53:35Z</td>\n",
       "      <td>Ethereum’s (ETH) Ethereum Foundation is initia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'id': None, 'name': 'ReadWrite'}</td>\n",
       "      <td>Radek Zielinski</td>\n",
       "      <td>US Ethereum ETFs see net outflows on second tr...</td>\n",
       "      <td>On their second day of trading, United States-...</td>\n",
       "      <td>https://readwrite.com/us-ethereum-etfs-see-net...</td>\n",
       "      <td>https://readwrite.com/wp-content/uploads/2024/...</td>\n",
       "      <td>2024-07-25T16:37:37Z</td>\n",
       "      <td>On their second day of trading, United States-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'id': None, 'name': 'ReadWrite'}</td>\n",
       "      <td>Alvin Hemedez</td>\n",
       "      <td>Ethereum Price Prediction July 2024 and a New ...</td>\n",
       "      <td>Ethereum is currently facing strong selling pr...</td>\n",
       "      <td>https://readwrite.com/ethereum-price-predictio...</td>\n",
       "      <td>https://readwrite.com/wp-content/uploads/2024/...</td>\n",
       "      <td>2024-07-10T08:39:19Z</td>\n",
       "      <td>Ethereum is currently facing strong selling pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'id': None, 'name': 'ReadWrite'}</td>\n",
       "      <td>Radek Zielinski</td>\n",
       "      <td>Spot Ethereum ETFs Set to Launch in US on July 23</td>\n",
       "      <td>The Securities and Exchange Commission (SEC) h...</td>\n",
       "      <td>https://readwrite.com/spot-ethereum-etfs-set-t...</td>\n",
       "      <td>https://readwrite.com/wp-content/uploads/2024/...</td>\n",
       "      <td>2024-07-23T17:32:29Z</td>\n",
       "      <td>The Securities and Exchange Commission (SEC)ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'id': None, 'name': 'ReadWrite'}</td>\n",
       "      <td>Radek Zielinski</td>\n",
       "      <td>Ethereum co-founder urges broader political pe...</td>\n",
       "      <td>Vitalik Buterin, Ethereum’s (ETH) co-founder, ...</td>\n",
       "      <td>https://readwrite.com/ethereum-co-founder-vita...</td>\n",
       "      <td>https://readwrite.com/wp-content/uploads/2024/...</td>\n",
       "      <td>2024-07-18T21:06:40Z</td>\n",
       "      <td>Vitalik Buterin, Ethereum’s (ETH) co-founder, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>{'id': None, 'name': 'ReadWrite'}</td>\n",
       "      <td>Alvin Hemedez</td>\n",
       "      <td>Trending Base Meme Coin Predicted to Surge 10x...</td>\n",
       "      <td>Experts predict that Toshi, one of the most po...</td>\n",
       "      <td>https://readwrite.com/trending-base-meme-coin-...</td>\n",
       "      <td>https://readwrite.com/wp-content/uploads/2024/...</td>\n",
       "      <td>2024-07-28T13:04:57Z</td>\n",
       "      <td>Experts predict that Toshi, one of the most po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>{'id': None, 'name': 'ReadWrite'}</td>\n",
       "      <td>Alvin Hemedez</td>\n",
       "      <td>Andrew Tate Pumps $Daddy Token – Could We See ...</td>\n",
       "      <td>Andrew Tate is once again in the spotlight, th...</td>\n",
       "      <td>https://readwrite.com/andrew-tate-pumps-daddy-...</td>\n",
       "      <td>https://readwrite.com/wp-content/uploads/2024/...</td>\n",
       "      <td>2024-06-30T17:36:23Z</td>\n",
       "      <td>Andrew Tate is once again in the spotlight, th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>{'id': None, 'name': 'ReadWrite'}</td>\n",
       "      <td>Alvin Hemedez</td>\n",
       "      <td>Fastest-Growing P2E Meme Coin Gem with 100x Po...</td>\n",
       "      <td>Despite a slight dip in the overall market, Pl...</td>\n",
       "      <td>https://readwrite.com/fastest-growing-p2e-meme...</td>\n",
       "      <td>https://readwrite.com/wp-content/uploads/2024/...</td>\n",
       "      <td>2024-06-30T13:27:56Z</td>\n",
       "      <td>Despite a slight dip in the overall market, Pl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>{'id': None, 'name': 'Forbes'}</td>\n",
       "      <td>Korok Ray, Contributor, \\n Korok Ray, Contribu...</td>\n",
       "      <td>Is The Ethereum ETF Good For Bitcoin?</td>\n",
       "      <td>his surprised me along with many others (and m...</td>\n",
       "      <td>https://www.forbes.com/sites/digital-assets/20...</td>\n",
       "      <td>https://imageio.forbes.com/specials-images/ima...</td>\n",
       "      <td>2024-07-24T15:32:23Z</td>\n",
       "      <td>Visual representation of the digital Cryptocur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>{'id': None, 'name': 'Forbes'}</td>\n",
       "      <td>Clem Chambers, Senior Contributor, \\n Clem Cha...</td>\n",
       "      <td>Will Ethereum Catch Up With Bitcoin?</td>\n",
       "      <td>Since the bottom of the crypto winter, bitcoin...</td>\n",
       "      <td>https://www.forbes.com/sites/digital-assets/20...</td>\n",
       "      <td>https://imageio.forbes.com/specials-images/ima...</td>\n",
       "      <td>2024-07-23T14:33:07Z</td>\n",
       "      <td>getty\\r\\ngetty\\r\\nWhen two linked assets diver...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               source  \\\n",
       "0   {'id': None, 'name': 'ReadWrite'}   \n",
       "1   {'id': None, 'name': 'ReadWrite'}   \n",
       "2   {'id': None, 'name': 'ReadWrite'}   \n",
       "3   {'id': None, 'name': 'ReadWrite'}   \n",
       "4   {'id': None, 'name': 'ReadWrite'}   \n",
       "..                                ...   \n",
       "95  {'id': None, 'name': 'ReadWrite'}   \n",
       "96  {'id': None, 'name': 'ReadWrite'}   \n",
       "97  {'id': None, 'name': 'ReadWrite'}   \n",
       "98     {'id': None, 'name': 'Forbes'}   \n",
       "99     {'id': None, 'name': 'Forbes'}   \n",
       "\n",
       "                                               author  \\\n",
       "0                                     Radek Zielinski   \n",
       "1                                     Radek Zielinski   \n",
       "2                                       Alvin Hemedez   \n",
       "3                                     Radek Zielinski   \n",
       "4                                     Radek Zielinski   \n",
       "..                                                ...   \n",
       "95                                      Alvin Hemedez   \n",
       "96                                      Alvin Hemedez   \n",
       "97                                      Alvin Hemedez   \n",
       "98  Korok Ray, Contributor, \\n Korok Ray, Contribu...   \n",
       "99  Clem Chambers, Senior Contributor, \\n Clem Cha...   \n",
       "\n",
       "                                                title  \\\n",
       "0   Ethereum launches $2M ‘Attackathon’ security a...   \n",
       "1   US Ethereum ETFs see net outflows on second tr...   \n",
       "2   Ethereum Price Prediction July 2024 and a New ...   \n",
       "3   Spot Ethereum ETFs Set to Launch in US on July 23   \n",
       "4   Ethereum co-founder urges broader political pe...   \n",
       "..                                                ...   \n",
       "95  Trending Base Meme Coin Predicted to Surge 10x...   \n",
       "96  Andrew Tate Pumps $Daddy Token – Could We See ...   \n",
       "97  Fastest-Growing P2E Meme Coin Gem with 100x Po...   \n",
       "98              Is The Ethereum ETF Good For Bitcoin?   \n",
       "99               Will Ethereum Catch Up With Bitcoin?   \n",
       "\n",
       "                                          description  \\\n",
       "0   Ethereum’s (ETH) Ethereum Foundation is initia...   \n",
       "1   On their second day of trading, United States-...   \n",
       "2   Ethereum is currently facing strong selling pr...   \n",
       "3   The Securities and Exchange Commission (SEC) h...   \n",
       "4   Vitalik Buterin, Ethereum’s (ETH) co-founder, ...   \n",
       "..                                                ...   \n",
       "95  Experts predict that Toshi, one of the most po...   \n",
       "96  Andrew Tate is once again in the spotlight, th...   \n",
       "97  Despite a slight dip in the overall market, Pl...   \n",
       "98  his surprised me along with many others (and m...   \n",
       "99  Since the bottom of the crypto winter, bitcoin...   \n",
       "\n",
       "                                                  url  \\\n",
       "0   https://readwrite.com/ethereum-launches-2m-att...   \n",
       "1   https://readwrite.com/us-ethereum-etfs-see-net...   \n",
       "2   https://readwrite.com/ethereum-price-predictio...   \n",
       "3   https://readwrite.com/spot-ethereum-etfs-set-t...   \n",
       "4   https://readwrite.com/ethereum-co-founder-vita...   \n",
       "..                                                ...   \n",
       "95  https://readwrite.com/trending-base-meme-coin-...   \n",
       "96  https://readwrite.com/andrew-tate-pumps-daddy-...   \n",
       "97  https://readwrite.com/fastest-growing-p2e-meme...   \n",
       "98  https://www.forbes.com/sites/digital-assets/20...   \n",
       "99  https://www.forbes.com/sites/digital-assets/20...   \n",
       "\n",
       "                                           urlToImage           publishedAt  \\\n",
       "0   https://readwrite.com/wp-content/uploads/2024/...  2024-07-09T09:53:35Z   \n",
       "1   https://readwrite.com/wp-content/uploads/2024/...  2024-07-25T16:37:37Z   \n",
       "2   https://readwrite.com/wp-content/uploads/2024/...  2024-07-10T08:39:19Z   \n",
       "3   https://readwrite.com/wp-content/uploads/2024/...  2024-07-23T17:32:29Z   \n",
       "4   https://readwrite.com/wp-content/uploads/2024/...  2024-07-18T21:06:40Z   \n",
       "..                                                ...                   ...   \n",
       "95  https://readwrite.com/wp-content/uploads/2024/...  2024-07-28T13:04:57Z   \n",
       "96  https://readwrite.com/wp-content/uploads/2024/...  2024-06-30T17:36:23Z   \n",
       "97  https://readwrite.com/wp-content/uploads/2024/...  2024-06-30T13:27:56Z   \n",
       "98  https://imageio.forbes.com/specials-images/ima...  2024-07-24T15:32:23Z   \n",
       "99  https://imageio.forbes.com/specials-images/ima...  2024-07-23T14:33:07Z   \n",
       "\n",
       "                                              content  \n",
       "0   Ethereum’s (ETH) Ethereum Foundation is initia...  \n",
       "1   On their second day of trading, United States-...  \n",
       "2   Ethereum is currently facing strong selling pr...  \n",
       "3   The Securities and Exchange Commission (SEC)ha...  \n",
       "4   Vitalik Buterin, Ethereum’s (ETH) co-founder, ...  \n",
       "..                                                ...  \n",
       "95  Experts predict that Toshi, one of the most po...  \n",
       "96  Andrew Tate is once again in the spotlight, th...  \n",
       "97  Despite a slight dip in the overall market, Pl...  \n",
       "98  Visual representation of the digital Cryptocur...  \n",
       "99  getty\\r\\ngetty\\r\\nWhen two linked assets diver...  \n",
       "\n",
       "[100 rows x 8 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fetch news data from NewsAPI, process it to extract relevant information, and clean the data\n",
    "\n",
    "API_KEY = os.getenv('API_KEY_News_API')\n",
    "BASE_URL = os.getenv('BASE_URL_News_API')\n",
    "\n",
    "def fetch_news(query, from_date, to_date):\n",
    "    params = {\n",
    "        'q': query,\n",
    "        'from': from_date,\n",
    "        'to': to_date,\n",
    "        'apiKey': API_KEY,\n",
    "        'language': 'en'\n",
    "    }\n",
    "    response = requests.get(BASE_URL, params=params)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        articles = data.get('articles', [])\n",
    "        return articles\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code}\")\n",
    "        print(response.text)\n",
    "        return []\n",
    "\n",
    "def save_news_to_csv(articles, filename):\n",
    "    df = pd.DataFrame(articles)\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"Saved {len(df)} articles to {filename}\")\n",
    "\n",
    "current_date = datetime.datetime.now()\n",
    "from_date = (current_date - timedelta(days=30)).strftime('%Y-%m-%d')\n",
    "to_date = (current_date).strftime('%Y-%m-%d')\n",
    "articles = fetch_news('Ethereum', from_date, to_date)\n",
    "news_api = pd.DataFrame(articles)\n",
    "news_api\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch news from Google News\n",
    "\n",
    "\n",
    "def fetch_all_ethereum_news(api_key, query=\"Ethereum\", results_per_page=10):\n",
    "    articles = []\n",
    "    start = 0\n",
    "    \n",
    "    while True:\n",
    "        params = {\n",
    "            \"engine\": \"google\",\n",
    "            \"q\": query,\n",
    "            \"tbm\": \"nws\",\n",
    "            \"api_key\": api_key,\n",
    "            \"num\": results_per_page,\n",
    "            \"start\": start\n",
    "        }\n",
    "        \n",
    "        search = GoogleSearch(params)\n",
    "        results = search.get_dict()\n",
    "        \n",
    "        news_results = results.get(\"news_results\", [])\n",
    "        if not news_results:\n",
    "            break\n",
    "        \n",
    "        for news in news_results:\n",
    "            article = {\n",
    "                \"title\": news.get(\"title\"),\n",
    "                \"snippet\": news.get(\"snippet\"),\n",
    "                \"date\": news.get(\"date\"),\n",
    "                \"link\": news.get(\"link\")\n",
    "            }\n",
    "            articles.append(article)\n",
    "        \n",
    "        start += results_per_page\n",
    "    \n",
    "    return pd.DataFrame(articles)\n",
    "\n",
    "\n",
    "api_key = os.getenv('api_key_google')\n",
    "news_df = fetch_all_ethereum_news(api_key)\n",
    "news_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Convert the publication time from Google News to a standard datetime format\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def convert_relative_time(time_str):\n",
    "\n",
    "    hour_pattern = re.compile(r'(\\d+)\\s*hour')\n",
    "    minute_pattern = re.compile(r'(\\d+)\\s*minute')\n",
    "    day_pattern = re.compile(r'(\\d+)\\s*day')\n",
    "    week_pattern = re.compile(r'(\\d+)\\s*week')\n",
    "    month_pattern = re.compile(r'(\\d+)\\s*month')\n",
    "\n",
    "    hours_match = hour_pattern.search(time_str)\n",
    "    minutes_match = minute_pattern.search(time_str)\n",
    "    days_match = day_pattern.search(time_str)\n",
    "    weeks_match = week_pattern.search(time_str)\n",
    "    months_match = month_pattern.search(time_str)\n",
    "\n",
    "    hours = int(hours_match.group(1)) if hours_match else 0\n",
    "    minutes = int(minutes_match.group(1)) if minutes_match else 0\n",
    "    days = int(days_match.group(1)) if days_match else 0\n",
    "    weeks = int(weeks_match.group(1)) if weeks_match else 0\n",
    "    months = int(months_match.group(1)) if months_match else 0\n",
    "\n",
    "\n",
    "    if hours > 0 or minutes > 0 or days > 0 or weeks > 0 or months > 0:\n",
    "        absolute_time = datetime.now() - timedelta(\n",
    "            hours=hours,\n",
    "            minutes=minutes,\n",
    "            days=days + weeks * 7\n",
    "        ) - timedelta(days=months * 30)\n",
    "        return absolute_time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    return time_str\n",
    "\n",
    "\n",
    "\n",
    "news_df['date'] = news_df['date'].apply(convert_relative_time)\n",
    "\n",
    "news_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "def remove_newlines(text):\n",
    "    return text.replace('\\n', ' ')\n",
    "\n",
    "news_df['snippet'] = news_df['snippet'].apply(remove_newlines)\n",
    "\n",
    "def convert_date(date_str):\n",
    "    for fmt in ('%Y-%m-%d %H:%M:%S', '%b %d, %Y'):\n",
    "        try:\n",
    "            return datetime.strptime(date_str, fmt).strftime('%Y-%m-%d %H:%M:%S')\n",
    "        except ValueError:\n",
    "            continue\n",
    "    return None\n",
    "\n",
    "news_df['date'] = news_df['date'].apply(convert_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Fetch OHLC price data from Binance API at minute intervals\n",
    "\n",
    "import datetime\n",
    "\n",
    "API_KEY = os.getenv('API_KEY_OHLC')\n",
    "BASE_URL = os.getenv('BASE_URL_OHLC')\n",
    "\n",
    "def get_binance_ohlc(symbol, interval, start_time, end_time):\n",
    "    params = {\n",
    "        'symbol': symbol,\n",
    "        'interval': interval,\n",
    "        'startTime': int(start_time.timestamp() * 1000),\n",
    "        'endTime': int(end_time.timestamp() * 1000),\n",
    "        'limit': 1000\n",
    "    }\n",
    "    response = requests.get(BASE_URL, params=params, headers={'X-MBX-APIKEY': API_KEY})\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code}\")\n",
    "        print(response.text)\n",
    "        return None\n",
    "\n",
    "def format_data(data):\n",
    "    formatted_data = []\n",
    "    for item in data:\n",
    "        timestamp = item[0] / 1000\n",
    "        date_time = datetime.datetime.fromtimestamp(timestamp)\n",
    "        formatted_data.append({\n",
    "            'datetime': date_time,\n",
    "            'open': float(item[1]),\n",
    "            'high': float(item[2]),\n",
    "            'low': float(item[3]),\n",
    "            'close': float(item[4])\n",
    "        })\n",
    "    return formatted_data\n",
    "\n",
    "time = []\n",
    "end_date = datetime.datetime.now()\n",
    "start_date = news['time'].min()\n",
    "\n",
    "while end_date > start_date:\n",
    "    start_interval = end_date - datetime.timedelta(minutes=1000)\n",
    "    data = get_binance_ohlc('ETHUSDT', '1m', start_interval, end_date)\n",
    "    \n",
    "    if data:\n",
    "        formatted_data = format_data(data)\n",
    "        time.extend(formatted_data)\n",
    "    \n",
    "    end_date = start_interval\n",
    "\n",
    "ohlc_df = pd.DataFrame(time)\n",
    "ohlc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Fetch OHLC price data from Binance API at hourly intervals\n",
    "\n",
    "API_KEY = os.getenv('API_KEY_OHLC')\n",
    "BASE_URL = os.getenv('BASE_URL_OHLC')\n",
    "\n",
    "def get_binance_ohlc(symbol, interval, start_time, end_time):\n",
    "    params = {\n",
    "        'symbol': symbol,\n",
    "        'interval': interval,\n",
    "        'startTime': int(start_time.timestamp() * 1000),\n",
    "        'endTime': int(end_time.timestamp() * 1000),\n",
    "        'limit': 10000\n",
    "    }\n",
    "    response = requests.get(BASE_URL, params=params, headers={'X-MBX-APIKEY': API_KEY})\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code}\")\n",
    "        print(response.text)\n",
    "        return None\n",
    "\n",
    "def format_data(data):\n",
    "    formatted_data = []\n",
    "    for item in data:\n",
    "        timestamp = item[0] / 1000\n",
    "        date_time = datetime.datetime.fromtimestamp(timestamp)\n",
    "        formatted_data.append({\n",
    "            'datetime': date_time,\n",
    "            'open': float(item[1]),\n",
    "            'high': float(item[2]),\n",
    "            'low': float(item[3]),\n",
    "            'close': float(item[4])\n",
    "        })\n",
    "    return formatted_data\n",
    "\n",
    "\n",
    "end_date = datetime.datetime.now()\n",
    "start_date = end_date - datetime.timedelta(days=40)\n",
    "\n",
    "data = get_binance_ohlc('ETHUSDT', '1h', start_date, end_date)\n",
    "\n",
    "if data:\n",
    "    formatted_data = format_data(data)\n",
    "    \n",
    "    ohlc_df_h = pd.DataFrame(formatted_data)\n",
    "else:\n",
    "    print(\"No data available or error in response.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch OHLC price data from Binance API at daily intervals\n",
    "\n",
    "API_KEY = os.getenv('API_KEY_OHLC')\n",
    "BASE_URL = os.getenv('BASE_URL_OHLC')\n",
    "\n",
    "def get_binance_ohlc(symbol, interval, start_time, end_time):\n",
    "    params = {\n",
    "        'symbol': symbol,\n",
    "        'interval': interval,\n",
    "        'startTime': int(start_time.timestamp() * 1000),\n",
    "        'endTime': int(end_time.timestamp() * 1000),\n",
    "        'limit': 10000\n",
    "    }\n",
    "    response = requests.get(BASE_URL, params=params, headers={'X-MBX-APIKEY': API_KEY})\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code}\")\n",
    "        print(response.text)\n",
    "        return None\n",
    "\n",
    "def format_data(data):\n",
    "    formatted_data = []\n",
    "    for item in data:\n",
    "        timestamp = item[0] / 1000\n",
    "        date_time = datetime.datetime.fromtimestamp(timestamp)\n",
    "        formatted_data.append({\n",
    "            'datetime': date_time,\n",
    "            'open': float(item[1]),\n",
    "            'high': float(item[2]),\n",
    "            'low': float(item[3]),\n",
    "            'close': float(item[4])\n",
    "        })\n",
    "    return formatted_data\n",
    "\n",
    "\n",
    "end_date = datetime.datetime.now() - datetime.timedelta(days=41)\n",
    "start_date = end_date - datetime.timedelta(days=720)\n",
    "\n",
    "data = get_binance_ohlc('ETHUSDT', '1d', start_date, end_date)\n",
    "\n",
    "if data:\n",
    "    formatted_data = format_data(data)\n",
    "    \n",
    "    ohlc_df_d = pd.DataFrame(formatted_data)\n",
    "else:\n",
    "    print(\"No data available or error in response.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohlc_df_d['datetime'] = pd.to_datetime(ohlc_df_d['datetime'])\n",
    "ohlc_df_d['datetime'] = ohlc_df_d['datetime'] - pd.Timedelta(hours=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "ohlc_df.to_sql(name = 'df_ohlc'\n",
    "            , con = engine\n",
    "            , index = False\n",
    "            , if_exists ='replace')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Rename columns in the DataFrame\n",
    "news.rename(columns={\"time\": \"timestamp\"}, inplace=True)\n",
    "ohlc_df.rename(columns={\"datetime\": \"timestamp\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "ohlc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Merge datasets based on the 'timestamp' column\n",
    "\n",
    "merged_df_1 = pd.merge_asof(news.sort_values('timestamp'), \n",
    "                          ohlc_df.sort_values('timestamp'), \n",
    "                          on='timestamp', \n",
    "                          direction='forward')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "merged_df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Add a label to the dataset to indicate which news articles are associated with price changes\n",
    "\n",
    "merged_df_1['price_difference'] = merged_df_1['close'] - merged_df_1['open']\n",
    "merged_df_1['label'] = (merged_df_1['price_difference'] > 0).astype(int)\n",
    "merged_df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "merged_df_1.rename(columns={\"text\":\"content\"},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Convert or adjust the time data to ensure consistency and alignment with other datasets\n",
    "\n",
    "ohlc_df = pd.concat([ohlc_df_h, ohlc_df_d], ignore_index=True)\n",
    "ohlc_df['timestamp'] = pd.to_datetime(ohlc_df['datetime'])\n",
    "ohlc_df['timestamp'] = ohlc_df['timestamp'].dt.tz_localize(None)\n",
    "ohlc_df.drop(columns=\"datetime\",inplace=True)\n",
    "ohlc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "eth_news_0['timestamp'] = pd.to_datetime(eth_news_0['publishedAt'])\n",
    "eth_news_0['timestamp'] = eth_news_0['timestamp'].dt.round('H')\n",
    "eth_news_0['timestamp'] = eth_news_0['timestamp'].dt.tz_localize(None)\n",
    "eth_news_0 = eth_news_0[[\"timestamp\",\"content\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "eth_news_1['timestamp'] = pd.to_datetime(eth_news_1['date'])\n",
    "eth_news_1['timestamp'] = eth_news_1['timestamp'].dt.round('H')\n",
    "eth_news_1['timestamp'] = eth_news_1['timestamp'].dt.tz_localize(None)\n",
    "eth_news_1.rename(columns={'snippet': 'content'},inplace=True)\n",
    "eth_news_1 = eth_news_1[[\"timestamp\",\"content\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "eth_news_2['timestamp'] = pd.to_datetime(eth_news_2['date'])\n",
    "eth_news_2['timestamp'] = eth_news_2['timestamp'].dt.round('H')\n",
    "eth_news_2['timestamp'] = eth_news_2['timestamp'].dt.tz_localize(None)\n",
    "eth_news_2.rename(columns={'snippet': 'content'},inplace=True)\n",
    "eth_news_2 = eth_news_2[[\"timestamp\",\"content\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "eth_news = pd.concat([eth_news_0, eth_news_1,eth_news_2], ignore_index=True)\n",
    "eth_news.dropna(inplace=True)\n",
    "eth_news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Merge datasets based on the 'timestamp' column\n",
    "\n",
    "merged_df_2 = pd.merge_asof(eth_news.sort_values('timestamp'), \n",
    "                          ohlc_df.sort_values('timestamp'), \n",
    "                          on='timestamp', \n",
    "                          direction='forward')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Add a label to the dataset to indicate which news articles are associated with price changes\n",
    "\n",
    "merged_df_2['price_difference'] = merged_df_2['close'] - merged_df_2['open']\n",
    "merged_df_2['label'] = (merged_df_2['price_difference'] > 0).astype(int)\n",
    "merged_df_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Concatenate two merged datasets to create a unified dataset\n",
    "\n",
    "df = pd.concat([merged_df_1,merged_df_2],ignore_index=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Store the final dataset in the database\n",
    "\n",
    "df.to_sql(name = 'df_teleg'\n",
    "            , con = engine\n",
    "            , index = False\n",
    "            , if_exists ='replace')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Function to tokenize news articles into individual words or tokens\n",
    "\n",
    "news_texts = df['content'].tolist()\n",
    "labels = df['label'].tolist()\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding='max_length', truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Tokenize news articles to split text into individual words or tokens\n",
    "\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(news_texts, labels, test_size=0.1)\n",
    "\n",
    "\n",
    "train_dataset = Dataset.from_dict({'text': train_texts, 'label': train_labels})\n",
    "val_dataset = Dataset.from_dict({'text': val_texts, 'label': val_labels})\n",
    "\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "train_dataset = train_dataset.rename_column('label', 'labels')\n",
    "val_dataset = val_dataset.rename_column('label', 'labels')\n",
    "\n",
    "train_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "val_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Import the model architecture for training\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Set up training arguments and initialize the trainer for model training\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy='epoch',\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    logits = p.predictions\n",
    "    labels = p.label_ids\n",
    "    predictions = logits.argmax(axis=-1)\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    return {'accuracy': accuracy}\n",
    "\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Train the model using the training data and evaluate its performance on the validation set\n",
    "\n",
    "trainer.train()\n",
    "eval_results = trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model to a file for future use or deployment\n",
    "\n",
    "trainer.save_model('./final_model')\n",
    "tokenizer.save_pretrained('./final_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
