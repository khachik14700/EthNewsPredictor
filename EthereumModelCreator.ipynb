{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-28T16:58:51.819715Z",
     "start_time": "2024-07-28T16:58:45.457394Z"
    },
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "#Import necessary libraries\n",
    "\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from psycopg2.extras import execute_batch\n",
    "import psycopg2\n",
    "import requests\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime, timedelta\n",
    "import datetime\n",
    "import re\n",
    "from serpapi import GoogleSearch\n",
    "from datasets import Dataset\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException, WebDriverException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "#Connecting to the database\n",
    "\n",
    "load_dotenv()\n",
    "host = 'localhost'\n",
    "port = 5432 \n",
    "user = 'postgres'\n",
    "password = os.getenv('PASSWORD_DB')\n",
    "db = 'postgres' \n",
    "conn = psycopg2.connect(host=host, port=port, user=user, password=password, dbname=db)\n",
    "connection_string = f\"postgresql://{user}:{password}@{host}/{db}\"\n",
    "engine = create_engine(connection_string)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scrape news from CoinDesk\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "\n",
    "\n",
    "service = Service(ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "links = []\n",
    "xpaths = []\n",
    "\n",
    "for i in range(371):\n",
    "    base_url = f'https://www.coindesk.com/tag/ethereum/{i}/'\n",
    "    driver.get(base_url)\n",
    "    print(i)\n",
    "\n",
    "    for i in range(1,11):\n",
    "        xpaths.append(f'//*[@id=\"fusion-app\"]/div/div/div/main/div[2]/div/div[1]/div[{i}]') \n",
    "\n",
    "    for xpath in xpaths:\n",
    "        try:\n",
    "            articles_container = driver.find_element(By.XPATH, xpath)\n",
    "        \n",
    "            article_elements = articles_container.find_elements(By.XPATH, './/a')\n",
    "        \n",
    "            for article in article_elements:\n",
    "                href = article.get_attribute('href')\n",
    "                if href and href.startswith(\"https://www.coindesk.com/\"):\n",
    "                    links.append(href)\n",
    "        except Exception as e:\n",
    "            print(f\"Error finding elements with XPath {xpath}: {e}\")\n",
    "\n",
    "    for i, link in enumerate(links, start=1):\n",
    "        print(f\"Link {i}: {link}\")\n",
    "\n",
    "    xpaths.clear()\n",
    "\n",
    "\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_date(url):\n",
    "    return bool(re.search(r'\\d{4}/\\d{2}/\\d{2}', url))\n",
    "\n",
    "links_step_2 = [url for url in links if has_date(url)]\n",
    "links_end = list(set(links_step_2))\n",
    "links_end\n",
    "\n",
    "links_df= pd.DataFrame(links_end)\n",
    "links_df.to_sql(name = 'links_df'\n",
    "            , con = engine\n",
    "            , index = False\n",
    "            , if_exists ='replace')\n",
    "links_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links_dic = {\"text\": [], \"time\": []}\n",
    "\n",
    "def get_content(link):\n",
    "    try:\n",
    "        response = requests.get(link)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "\n",
    "        base_content = soup.find(attrs={\"data-module-name\": \"article-body-no-right-rail\"})\n",
    "        if base_content:\n",
    "            paragraphs = base_content.find_all('p')\n",
    "            all_text = \"\\n\".join(paragraph.get_text() for paragraph in paragraphs)\n",
    "        else:\n",
    "            all_text = \"Content not found\"\n",
    "\n",
    "\n",
    "        time_element = soup.find(class_=\"iOUkmj\")\n",
    "        time = time_element.get_text() if time_element else \"Time not found\"\n",
    "\n",
    "        links_dic['text'].append(all_text)\n",
    "        links_dic[\"time\"].append(time)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching content from {link}: {e}\")\n",
    "\n",
    "def get_contents(page_links):\n",
    "    for page_link in tqdm(page_links, desc=\"Fetching content\"):\n",
    "        get_content(page_link)\n",
    "\n",
    "\n",
    "get_contents(links_end)\n",
    "\n",
    "\n",
    "news = pd.DataFrame(links_dic)\n",
    "news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning and correcting data fetched from CoinDesk\n",
    "\n",
    "news = news[~news['text'].str.contains('Error fetching content from')]\n",
    "\n",
    "def convert_date(date_str):\n",
    "    date_str_cleaned = date_str.split(' UTC')[0].strip()\n",
    "    \n",
    "    date_str_cleaned = date_str_cleaned.replace('p.m.', 'PM').replace('a.m.', 'AM')\n",
    "    \n",
    "    formats = [\n",
    "        '%b %d, %Y at %I:%M %p',\n",
    "        '%b %d, %Y at %H:%M %p'\n",
    "    ]\n",
    "    \n",
    "    for fmt in formats:\n",
    "        try:\n",
    "            date_obj = datetime.strptime(date_str_cleaned, fmt)\n",
    "            return date_obj.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        except ValueError:\n",
    "            continue\n",
    "    return None\n",
    "\n",
    "news['time'] = news['time'].apply(convert_date)\n",
    "\n",
    "news.dropna(inplace=True)\n",
    "\n",
    "news['time'] = pd.to_datetime(news['time'], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Fetch news data from NewsAPI, process it to extract relevant information, and clean the data\n",
    "\n",
    "API_KEY = os.getenv('API_KEY_News_API')\n",
    "BASE_URL = os.getenv('BASE_URL_News_API')\n",
    "\n",
    "def fetch_news(query, from_date, to_date):\n",
    "    params = {\n",
    "        'q': query,\n",
    "        'from': from_date,\n",
    "        'to': to_date,\n",
    "        'apiKey': API_KEY,\n",
    "        'language': 'en'\n",
    "    }\n",
    "    response = requests.get(BASE_URL, params=params)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        articles = data.get('articles', [])\n",
    "        return articles\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code}\")\n",
    "        print(response.text)\n",
    "        return []\n",
    "\n",
    "def save_news_to_csv(articles, filename):\n",
    "    df = pd.DataFrame(articles)\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"Saved {len(df)} articles to {filename}\")\n",
    "\n",
    "current_date = datetime.datetime.now()\n",
    "from_date = (current_date - timedelta(days=30)).strftime('%Y-%m-%d')\n",
    "to_date = (current_date).strftime('%Y-%m-%d')\n",
    "articles = fetch_news('Ethereum', from_date, to_date)\n",
    "news_api = pd.DataFrame(articles)\n",
    "news_api\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch news from Google News\n",
    "\n",
    "\n",
    "def fetch_all_ethereum_news(api_key, query=\"Ethereum\", results_per_page=10):\n",
    "    articles = []\n",
    "    start = 0\n",
    "    \n",
    "    while True:\n",
    "        params = {\n",
    "            \"engine\": \"google\",\n",
    "            \"q\": query,\n",
    "            \"tbm\": \"nws\",\n",
    "            \"api_key\": api_key,\n",
    "            \"num\": results_per_page,\n",
    "            \"start\": start\n",
    "        }\n",
    "        \n",
    "        search = GoogleSearch(params)\n",
    "        results = search.get_dict()\n",
    "        \n",
    "        news_results = results.get(\"news_results\", [])\n",
    "        if not news_results:\n",
    "            break\n",
    "        \n",
    "        for news in news_results:\n",
    "            article = {\n",
    "                \"title\": news.get(\"title\"),\n",
    "                \"snippet\": news.get(\"snippet\"),\n",
    "                \"date\": news.get(\"date\"),\n",
    "                \"link\": news.get(\"link\")\n",
    "            }\n",
    "            articles.append(article)\n",
    "        \n",
    "        start += results_per_page\n",
    "    \n",
    "    return pd.DataFrame(articles)\n",
    "\n",
    "\n",
    "api_key = os.getenv('api_key_google')\n",
    "news_df = fetch_all_ethereum_news(api_key)\n",
    "news_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Convert the publication time from Google News to a standard datetime format\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def convert_relative_time(time_str):\n",
    "\n",
    "    hour_pattern = re.compile(r'(\\d+)\\s*hour')\n",
    "    minute_pattern = re.compile(r'(\\d+)\\s*minute')\n",
    "    day_pattern = re.compile(r'(\\d+)\\s*day')\n",
    "    week_pattern = re.compile(r'(\\d+)\\s*week')\n",
    "    month_pattern = re.compile(r'(\\d+)\\s*month')\n",
    "\n",
    "    hours_match = hour_pattern.search(time_str)\n",
    "    minutes_match = minute_pattern.search(time_str)\n",
    "    days_match = day_pattern.search(time_str)\n",
    "    weeks_match = week_pattern.search(time_str)\n",
    "    months_match = month_pattern.search(time_str)\n",
    "\n",
    "    hours = int(hours_match.group(1)) if hours_match else 0\n",
    "    minutes = int(minutes_match.group(1)) if minutes_match else 0\n",
    "    days = int(days_match.group(1)) if days_match else 0\n",
    "    weeks = int(weeks_match.group(1)) if weeks_match else 0\n",
    "    months = int(months_match.group(1)) if months_match else 0\n",
    "\n",
    "\n",
    "    if hours > 0 or minutes > 0 or days > 0 or weeks > 0 or months > 0:\n",
    "        absolute_time = datetime.now() - timedelta(\n",
    "            hours=hours,\n",
    "            minutes=minutes,\n",
    "            days=days + weeks * 7\n",
    "        ) - timedelta(days=months * 30)\n",
    "        return absolute_time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    return time_str\n",
    "\n",
    "\n",
    "\n",
    "news_df['date'] = news_df['date'].apply(convert_relative_time)\n",
    "\n",
    "news_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "def remove_newlines(text):\n",
    "    return text.replace('\\n', ' ')\n",
    "\n",
    "news_df['snippet'] = news_df['snippet'].apply(remove_newlines)\n",
    "\n",
    "def convert_date(date_str):\n",
    "    for fmt in ('%Y-%m-%d %H:%M:%S', '%b %d, %Y'):\n",
    "        try:\n",
    "            return datetime.strptime(date_str, fmt).strftime('%Y-%m-%d %H:%M:%S')\n",
    "        except ValueError:\n",
    "            continue\n",
    "    return None\n",
    "\n",
    "news_df['date'] = news_df['date'].apply(convert_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Fetch OHLC price data from Binance API at minute intervals\n",
    "\n",
    "import datetime\n",
    "\n",
    "API_KEY = os.getenv('API_KEY_OHLC')\n",
    "BASE_URL = os.getenv('BASE_URL_OHLC')\n",
    "\n",
    "def get_binance_ohlc(symbol, interval, start_time, end_time):\n",
    "    params = {\n",
    "        'symbol': symbol,\n",
    "        'interval': interval,\n",
    "        'startTime': int(start_time.timestamp() * 1000),\n",
    "        'endTime': int(end_time.timestamp() * 1000),\n",
    "        'limit': 1000\n",
    "    }\n",
    "    response = requests.get(BASE_URL, params=params, headers={'X-MBX-APIKEY': API_KEY})\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code}\")\n",
    "        print(response.text)\n",
    "        return None\n",
    "\n",
    "def format_data(data):\n",
    "    formatted_data = []\n",
    "    for item in data:\n",
    "        timestamp = item[0] / 1000\n",
    "        date_time = datetime.datetime.fromtimestamp(timestamp)\n",
    "        formatted_data.append({\n",
    "            'datetime': date_time,\n",
    "            'open': float(item[1]),\n",
    "            'high': float(item[2]),\n",
    "            'low': float(item[3]),\n",
    "            'close': float(item[4])\n",
    "        })\n",
    "    return formatted_data\n",
    "\n",
    "time = []\n",
    "end_date = datetime.datetime.now()\n",
    "start_date = news['time'].min()\n",
    "\n",
    "while end_date > start_date:\n",
    "    start_interval = end_date - datetime.timedelta(minutes=1000)\n",
    "    data = get_binance_ohlc('ETHUSDT', '1m', start_interval, end_date)\n",
    "    \n",
    "    if data:\n",
    "        formatted_data = format_data(data)\n",
    "        time.extend(formatted_data)\n",
    "    \n",
    "    end_date = start_interval\n",
    "\n",
    "ohlc_df = pd.DataFrame(time)\n",
    "ohlc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Fetch OHLC price data from Binance API at hourly intervals\n",
    "\n",
    "API_KEY = os.getenv('API_KEY_OHLC')\n",
    "BASE_URL = os.getenv('BASE_URL_OHLC')\n",
    "\n",
    "def get_binance_ohlc(symbol, interval, start_time, end_time):\n",
    "    params = {\n",
    "        'symbol': symbol,\n",
    "        'interval': interval,\n",
    "        'startTime': int(start_time.timestamp() * 1000),\n",
    "        'endTime': int(end_time.timestamp() * 1000),\n",
    "        'limit': 10000\n",
    "    }\n",
    "    response = requests.get(BASE_URL, params=params, headers={'X-MBX-APIKEY': API_KEY})\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code}\")\n",
    "        print(response.text)\n",
    "        return None\n",
    "\n",
    "def format_data(data):\n",
    "    formatted_data = []\n",
    "    for item in data:\n",
    "        timestamp = item[0] / 1000\n",
    "        date_time = datetime.datetime.fromtimestamp(timestamp)\n",
    "        formatted_data.append({\n",
    "            'datetime': date_time,\n",
    "            'open': float(item[1]),\n",
    "            'high': float(item[2]),\n",
    "            'low': float(item[3]),\n",
    "            'close': float(item[4])\n",
    "        })\n",
    "    return formatted_data\n",
    "\n",
    "\n",
    "end_date = datetime.datetime.now()\n",
    "start_date = end_date - datetime.timedelta(days=40)\n",
    "\n",
    "data = get_binance_ohlc('ETHUSDT', '1h', start_date, end_date)\n",
    "\n",
    "if data:\n",
    "    formatted_data = format_data(data)\n",
    "    \n",
    "    ohlc_df_h = pd.DataFrame(formatted_data)\n",
    "else:\n",
    "    print(\"No data available or error in response.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch OHLC price data from Binance API at daily intervals\n",
    "\n",
    "API_KEY = os.getenv('API_KEY_OHLC')\n",
    "BASE_URL = os.getenv('BASE_URL_OHLC')\n",
    "\n",
    "def get_binance_ohlc(symbol, interval, start_time, end_time):\n",
    "    params = {\n",
    "        'symbol': symbol,\n",
    "        'interval': interval,\n",
    "        'startTime': int(start_time.timestamp() * 1000),\n",
    "        'endTime': int(end_time.timestamp() * 1000),\n",
    "        'limit': 10000\n",
    "    }\n",
    "    response = requests.get(BASE_URL, params=params, headers={'X-MBX-APIKEY': API_KEY})\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code}\")\n",
    "        print(response.text)\n",
    "        return None\n",
    "\n",
    "def format_data(data):\n",
    "    formatted_data = []\n",
    "    for item in data:\n",
    "        timestamp = item[0] / 1000\n",
    "        date_time = datetime.datetime.fromtimestamp(timestamp)\n",
    "        formatted_data.append({\n",
    "            'datetime': date_time,\n",
    "            'open': float(item[1]),\n",
    "            'high': float(item[2]),\n",
    "            'low': float(item[3]),\n",
    "            'close': float(item[4])\n",
    "        })\n",
    "    return formatted_data\n",
    "\n",
    "\n",
    "end_date = datetime.datetime.now() - datetime.timedelta(days=41)\n",
    "start_date = end_date - datetime.timedelta(days=720)\n",
    "\n",
    "data = get_binance_ohlc('ETHUSDT', '1d', start_date, end_date)\n",
    "\n",
    "if data:\n",
    "    formatted_data = format_data(data)\n",
    "    \n",
    "    ohlc_df_d = pd.DataFrame(formatted_data)\n",
    "else:\n",
    "    print(\"No data available or error in response.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohlc_df_d['datetime'] = pd.to_datetime(ohlc_df_d['datetime'])\n",
    "ohlc_df_d['datetime'] = ohlc_df_d['datetime'] - pd.Timedelta(hours=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "ohlc_df.to_sql(name = 'df_ohlc'\n",
    "            , con = engine\n",
    "            , index = False\n",
    "            , if_exists ='replace')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Rename columns in the DataFrame\n",
    "news.rename(columns={\"time\": \"timestamp\"}, inplace=True)\n",
    "ohlc_df.rename(columns={\"datetime\": \"timestamp\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "ohlc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Merge datasets based on the 'timestamp' column\n",
    "\n",
    "merged_df_1 = pd.merge_asof(news.sort_values('timestamp'), \n",
    "                          ohlc_df.sort_values('timestamp'), \n",
    "                          on='timestamp', \n",
    "                          direction='forward')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "merged_df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Add a label to the dataset to indicate which news articles are associated with price changes\n",
    "\n",
    "merged_df_1['price_difference'] = merged_df_1['close'] - merged_df_1['open']\n",
    "merged_df_1['label'] = (merged_df_1['price_difference'] > 0).astype(int)\n",
    "merged_df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "merged_df_1.rename(columns={\"text\":\"content\"},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Convert or adjust the time data to ensure consistency and alignment with other datasets\n",
    "\n",
    "ohlc_df = pd.concat([ohlc_df_h, ohlc_df_d], ignore_index=True)\n",
    "ohlc_df['timestamp'] = pd.to_datetime(ohlc_df['datetime'])\n",
    "ohlc_df['timestamp'] = ohlc_df['timestamp'].dt.tz_localize(None)\n",
    "ohlc_df.drop(columns=\"datetime\",inplace=True)\n",
    "ohlc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "eth_news_0['timestamp'] = pd.to_datetime(eth_news_0['publishedAt'])\n",
    "eth_news_0['timestamp'] = eth_news_0['timestamp'].dt.round('H')\n",
    "eth_news_0['timestamp'] = eth_news_0['timestamp'].dt.tz_localize(None)\n",
    "eth_news_0 = eth_news_0[[\"timestamp\",\"content\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "eth_news_1['timestamp'] = pd.to_datetime(eth_news_1['date'])\n",
    "eth_news_1['timestamp'] = eth_news_1['timestamp'].dt.round('H')\n",
    "eth_news_1['timestamp'] = eth_news_1['timestamp'].dt.tz_localize(None)\n",
    "eth_news_1.rename(columns={'snippet': 'content'},inplace=True)\n",
    "eth_news_1 = eth_news_1[[\"timestamp\",\"content\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "eth_news_2['timestamp'] = pd.to_datetime(eth_news_2['date'])\n",
    "eth_news_2['timestamp'] = eth_news_2['timestamp'].dt.round('H')\n",
    "eth_news_2['timestamp'] = eth_news_2['timestamp'].dt.tz_localize(None)\n",
    "eth_news_2.rename(columns={'snippet': 'content'},inplace=True)\n",
    "eth_news_2 = eth_news_2[[\"timestamp\",\"content\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "eth_news = pd.concat([eth_news_0, eth_news_1,eth_news_2], ignore_index=True)\n",
    "eth_news.dropna(inplace=True)\n",
    "eth_news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Merge datasets based on the 'timestamp' column\n",
    "\n",
    "merged_df_2 = pd.merge_asof(eth_news.sort_values('timestamp'), \n",
    "                          ohlc_df.sort_values('timestamp'), \n",
    "                          on='timestamp', \n",
    "                          direction='forward')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Add a label to the dataset to indicate which news articles are associated with price changes\n",
    "\n",
    "merged_df_2['price_difference'] = merged_df_2['close'] - merged_df_2['open']\n",
    "merged_df_2['label'] = (merged_df_2['price_difference'] > 0).astype(int)\n",
    "merged_df_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Concatenate two merged datasets to create a unified dataset\n",
    "\n",
    "df = pd.concat([merged_df_1,merged_df_2],ignore_index=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Store the final dataset in the database\n",
    "\n",
    "df.to_sql(name = 'df_teleg'\n",
    "            , con = engine\n",
    "            , index = False\n",
    "            , if_exists ='replace')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Function to tokenize news articles into individual words or tokens\n",
    "\n",
    "news_texts = df['content'].tolist()\n",
    "labels = df['label'].tolist()\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding='max_length', truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Tokenize news articles to split text into individual words or tokens\n",
    "\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(news_texts, labels, test_size=0.1)\n",
    "\n",
    "\n",
    "train_dataset = Dataset.from_dict({'text': train_texts, 'label': train_labels})\n",
    "val_dataset = Dataset.from_dict({'text': val_texts, 'label': val_labels})\n",
    "\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "train_dataset = train_dataset.rename_column('label', 'labels')\n",
    "val_dataset = val_dataset.rename_column('label', 'labels')\n",
    "\n",
    "train_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "val_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Import the model architecture for training\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Set up training arguments and initialize the trainer for model training\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy='epoch',\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    logits = p.predictions\n",
    "    labels = p.label_ids\n",
    "    predictions = logits.argmax(axis=-1)\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    return {'accuracy': accuracy}\n",
    "\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Train the model using the training data and evaluate its performance on the validation set\n",
    "\n",
    "trainer.train()\n",
    "eval_results = trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model to a file for future use or deployment\n",
    "\n",
    "trainer.save_model('./final_model')\n",
    "tokenizer.save_pretrained('./final_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
